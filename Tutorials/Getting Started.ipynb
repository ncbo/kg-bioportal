{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cae906ae",
   "metadata": {},
   "source": [
    "# KG-Hub: Getting Started (from scratch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2255d9f4",
   "metadata": {},
   "source": [
    "This notebook serves as a walkthrough for creating a KG-Hub project, building a new graph, and using the graph for machine learning. Some familiarity with the command line, GitHub, and Python will be helpful. This notebook also assumes you're running in a Linux environment, but it should be informative even if you're on Windows or some other fancy operating system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61270fba",
   "metadata": {},
   "source": [
    "## KG-Hub basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dbf0e1",
   "metadata": {},
   "source": [
    "The purpose of Knowledge Graph Hub (KG-Hub) is to provide a platform for building knowledge graphs (KGs) by adopting a set of guidelines and design principles. The goal of KG-Hub is to serve as a collective resource to simplify the process of generating biological and biomedical KGs and thus reducing the barrier for entry to new participants.\n",
    "\n",
    "Each independent effort for building a KG is considered a KG-Hub project.\n",
    "\n",
    "Projects include a code repository and a storage location for each set of graph products. Repositories are generally on GitHub and storage is available on https://kg-hub.berkeleybop.io/.\n",
    "\n",
    "For example, the following are all KG-Hub project repositories and storage locations:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656d54e1",
   "metadata": {},
   "source": [
    "| **Name**    | **Repository**                                     | **Graphs**                                 |\n",
    "|-------------|----------------------------------------------------|--------------------------------------------|\n",
    "| KG-COVID-19 | https://github.com/Knowledge-Graph-Hub/kg-covid-19 | https://kg-hub.berkeleybop.io/kg-covid-19/ |\n",
    "| KG-IDG      | https://github.com/Knowledge-Graph-Hub/kg-idg      | https://kg-hub.berkeleybop.io/kg-idg/      |\n",
    "| KG-OBO      | https://github.com/Knowledge-Graph-Hub/kg-obo      | https://kg-hub.berkeleybop.io/kg-obo/      |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e47017",
   "metadata": {},
   "source": [
    "\n",
    "Each project *should*:\n",
    "* live in its own GitHub repository within the [Knowledge-Graph-Hub](https://github.com/Knowledge-Graph-Hub/) organization.\n",
    "* have enough code and/or configurations for Extract, Transform, and Load (ETL) to yield a reproducible product.\n",
    "* model data using the [Biolink Model](https://biolink.github.io/biolink-model/), where possible.\n",
    "* make use of ontologies from the [OBO Foundry](http://www.obofoundry.org/), where possible.\n",
    "* be responsible for the veracity of the datasets that they ingest \n",
    "* be responsible for keeping track of evidence and provenance for assertions in their KG.\n",
    "* provide their KG for download, following [semantic versioning guidelines](https://semver.org/).\n",
    "* provide their KG in the [KGX interchange format](https://github.com/biolink/kgx/blob/master/specification/kgx-format.md) in addition to their format of choice (e.g., n-triples).\n",
    "\n",
    "We also *highly recommend* including the following in the repository: \n",
    "* a README describing the intended purpose of the KG and its contributors\n",
    "* a License (as its own LICENSE file) \n",
    "* Contributing guidelines\n",
    "* a Code of Conduct\n",
    "* statements emphasizing how the KG and KG-Hub are open to the community for contributions as well as consumption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f316c89d",
   "metadata": {},
   "source": [
    "## Planning and setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40629ab9",
   "metadata": {},
   "source": [
    "You likely already found the KG-Hub project template repository - that's where this notebook is.\n",
    "If you found this someplace else, the template repository is https://github.com/Knowledge-Graph-Hub/kg-dtm-template.\n",
    "\n",
    "First, create a name for your project repository. In KG-Hub, most projects include **KG** somewhere, e.g., \"KG-Squid\" or \"KG-Mimivirus-Proteomics\".\n",
    "\n",
    "Make a new repository for your project, based on the template, in one of the following two ways:\n",
    "* [Follow the browser-based directions here.](https://docs.github.com/en/repositories/creating-and-managing-repositories/creating-a-repository-from-a-template)\n",
    "* Use the GitHub command line interface. This may be a preferable option if you're using a Windows command line and don't want to use a browser interface. Follow the [install instructions here](https://cli.github.com/manual/installation) as needed, then run:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c344976",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ~/ \n",
    "!gh repo create kg-project-name --public --clone --template https://github.com/Knowledge-Graph-Hub/kg-dtm-template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b288796f",
   "metadata": {},
   "source": [
    "Next, select a name for your project. It should resemble your repository name, though any dashes will need to be changed to underscores.\n",
    "\n",
    "Change the values below to your repository name so they may be used later in the walkthrough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00bc73cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_repo_name = \"kg-placeholder-name\"\n",
    "kg_project_name = \"kg_placeholder_name\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db72e359",
   "metadata": {},
   "source": [
    "Define some additional details so they may be used later as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9698510",
   "metadata": {},
   "outputs": [],
   "source": [
    "description = '' # A short description of the project\n",
    "long_description = '' # A slightly less short description of the project\n",
    "gh_name = '' # Your GitHub user name, assuming that you created the project repository in your own account.\n",
    "author_name = '' # Your name\n",
    "author_email = '' # Your email address"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45350acb",
   "metadata": {},
   "source": [
    "## Walking through the KG project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e25f2ea",
   "metadata": {},
   "source": [
    "Each KG project in KG-Hub is generally structured like this (with some omissions for clarity):\n",
    "```\n",
    "ðŸ“¦kg-project-name\n",
    " â”£ ðŸ“‚kg_project_name\n",
    " â”ƒ â”£ ðŸ“‚merge_utils\n",
    " â”ƒ â”ƒ â”— ðŸ“œmerge_kg.py - this produces the final, merged KG\n",
    " â”ƒ â”£ ðŸ“‚transform_utils - data source-specific transformation functions\n",
    " â”ƒ â”ƒ â”£ ðŸ“‚transform_one\n",
    " â”ƒ â”ƒ â”ƒ â”— ðŸ“œtransform_one.py\n",
    " â”ƒ â”ƒ â”£ ðŸ“‚transform_two\n",
    " â”ƒ â”ƒ â”ƒ â”— ðŸ“œtransform_two.py\n",
    " â”ƒ â”ƒ â”— ðŸ“œtransform.py - sets defaults for transform outputs\n",
    " â”ƒ â”£ ðŸ“‚utils - utilities and helper functions\n",
    " â”ƒ â”ƒ â”£ ðŸ“œdownload_utils.py\n",
    " â”ƒ â”ƒ â”£ ðŸ“œrobot_utils.py - utilities for working with the ROBOT tool\n",
    " â”ƒ â”ƒ â”— ðŸ“œtransform_utils.py\n",
    " â”ƒ â”£ ðŸ“œdownload.py \n",
    " â”ƒ â”— ðŸ“œtransform.py - sets up the individual transformations\n",
    " â”£ ðŸ“‚tests\n",
    " â”ƒ â”£ ðŸ“‚resources\n",
    " â”ƒ â”ƒ â”£ files required to run the tests\n",
    " â”ƒ â”£ various tests\n",
    " â”£ ðŸ“œLICENSE.txt\n",
    " â”£ ðŸ“œREADME.md - modify as needed\n",
    " â”£ ðŸ“œdownload.yaml - the download configuration file\n",
    " â”£ ðŸ“œmerge.yaml - the merge configuration file\n",
    " â”£ ðŸ“œrequirements.txt - empty by default, but add any new requirements here\n",
    " â”£ ðŸ“œrun.py - the main interface for downloading, transforming, and merging\n",
    " â”— ðŸ“œsetup.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a62dd5b",
   "metadata": {},
   "source": [
    "The general process of *defining* how to assemble a KG looks like this:\n",
    "1. Add data sources to `download.yaml`.\n",
    "2. Add a new transform to `transform.py` and in the `transform_utils` directory to handle the new data source. If the data source is already a set of KGX tsv node and edgelists, then it may only require a 'passthrough' tranform (i.e., files aren't modified but may be validated and moved). \n",
    "3. Modify `merge.yaml` to include the new sources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fea6ea0",
   "metadata": {},
   "source": [
    "The general process of *assembling* the KG looks like this. Even if you haven't changed much in the new project yet, these commands will still retrieve several files, transform them, and merge them into a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03e33772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:03<00:00,  1.58it/s]\n"
     ]
    }
   ],
   "source": [
    "!python run.py download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2842d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run.py transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacd7d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run.py merge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a06d5b",
   "metadata": {},
   "source": [
    "## Setting up your new KG project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3f7738",
   "metadata": {},
   "source": [
    "Before going any further, open the project in your favorite development environment. You will need to replace all instances of `project_name` with your project's name.\n",
    "\n",
    "You can also run the following script, assuming you've specified `kg_project_name` above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53440565",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$kg_project_name\"\n",
    "mv project_name/ $1\n",
    "find . -name \"*.py\" | xargs -n 1 sed -i -e \"s|project_name|$1|g\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7097a205",
   "metadata": {},
   "source": [
    "Next, update the `setup.py` file with some details about your project. You should only need to modify the first several lines of the `setup` block, as these define project metadata. Don't modify the value for `version` as that is defined elsewhere (specifically, in `project_name/__version__.py`). You may also need to change the value for `license` if you're using a license other than BSD-3. \n",
    "\n",
    "If you've defined metadata values in the \"Planning and setup\" section above, you may run the following, then copy and paste the result into your `setup.py` immediately under `setup(`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72537b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    name='kg_placeholder_name,\n",
      "    version=__version__,\n",
      "    description='',\n",
      "    long_description='',\n",
      "    url='https://github.com/Knowledge-Graph-Hub/kg-placeholder-name',\n",
      "    author='',\n",
      "    author_email='',\n",
      "    python_requires='>=3.7',\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print( \n",
    "f\"\"\"\n",
    "    name='{kg_project_name},\n",
    "    version=__version__,\n",
    "    description='{description}',\n",
    "    long_description='{long_description}',\n",
    "    url='https://github.com/Knowledge-Graph-Hub/{kg_repo_name}',\n",
    "    author='{author_name}',\n",
    "    author_email='{author_email}',\n",
    "    python_requires='>=3.7',\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264bc04e",
   "metadata": {},
   "source": [
    "Now it's all yours! Feel free to update the README, too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5adb6c",
   "metadata": {},
   "source": [
    "## KGX format basics and the KGX config files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2518f4c4",
   "metadata": {},
   "source": [
    "Two more steps remain before the KG is ready to be built: setting up the KGX configuration files (specifially, `download.yaml` and `merge.yaml`). You've likely noticed that the three primary stages in this pipeline are to download, transform, and merge. These two configuration files handle the first and last stages, respectively, but transforms are source-specific and each requires its own process. \n",
    "\n",
    "Our goal is to get all data in the same format - KGX tab-separated values - and adhering to the same data model. The next section will discuss the [Biolink Model](https://biolink.github.io/biolink-model/), but there isn't anything preventing you from reading about it now. In short, we need as much consistency as possible before combining sources into a single KG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6abc190",
   "metadata": {},
   "source": [
    "### The KGX format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4762d5af",
   "metadata": {},
   "source": [
    "[You can find an exhaustive specification for the KGX data format here.](https://github.com/biolink/kgx/blob/master/specification/kgx-format.md)\n",
    "\n",
    "If you're already familiar with RDF, triples, and the idea of a [graph data model](https://www.w3.org/TR/2004/REC-rdf-concepts-20040210/#section-data-model) then this will all appear quite simple. If not, just consider nodes to be things and edges to be the relationships between those things. For example, a node may be a farmer and an edge may be a specific connection to something else which may or may not be the same type of thing, e.g., \"Farmer Alphonse *grows* lentils\".\n",
    "\n",
    "Here are the key points about KGX:\n",
    "* Each graph consists of one node file and one edge file.\n",
    "* Both files are tab-delimited and have a single header line each.\n",
    "* Both files contain one record per line - one node or one edge.\n",
    "\n",
    "A node file generally looks something like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4f24bb",
   "metadata": {},
   "source": [
    "```\n",
    "id      category        name    description\n",
    "ENSEMBL:ENSG00000143933 biolink:Gene|biolink:NamedThing CALM2   calmodulin 2\n",
    "ENSEMBL:ENSG00000131089 biolink:Gene|biolink:NamedThing ARHGEF9 Cdc42 guanine nucleotide exchange factor 9\n",
    "ENSEMBL:ENSG00000147889 biolink:Gene|biolink:NamedThing CDKN2A  cyclin dependent kinase inhibitor 2A\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464d314e",
   "metadata": {},
   "source": [
    "Note that each value in the `id` column is a [CURIE](https://en.wikipedia.org/wiki/CURIE). It contains a prefix denoting the data source (in this case, ENSEMBL) and, after the colon, an identifier. Each node shown here also has two categories, with the | character separating items in each list. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9442a559",
   "metadata": {},
   "source": [
    "An edge file generally looks something like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c0ac03",
   "metadata": {},
   "source": [
    "```\n",
    "id      subject predicate       object\n",
    "urn:uuid:e99e9dd6-0b4f-416e-8c81-061b4a61711c   ENSEMBL:ENSP00000000233 biolink:interacts_with  ENSEMBL:ENSP0000027\n",
    "2298\n",
    "urn:uuid:a819d828-3df7-4384-8612-38a17e521320   ENSEMBL:ENSP00000000233 biolink:interacts_with  ENSEMBL:ENSP0000041\n",
    "8915\n",
    "urn:uuid:b412961f-8939-488a-bd37-6ae3bc7237b9   ENSEMBL:ENSP00000000233 biolink:interacts_with  ENSEMBL:ENSP0000035\n",
    "6737\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8d316e",
   "metadata": {},
   "source": [
    "Here, each `id` is actually a [Uniform Resource Name](https://en.wikipedia.org/wiki/Uniform_Resource_Name) - this is for consistency because the KG may contain a mix of relationships from other sources *and* newly-created connections. The crucial aspect is the set of *subject*, *predicate*, and *object*, or \"*S* has relationship *P* with *O*\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe50f4ec",
   "metadata": {},
   "source": [
    "### KGX config files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b826277",
   "metadata": {},
   "source": [
    "The KGX configuration files usually remain in the root of each project. Run this to see the download config, `download.yaml`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32ae5a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# This file is a list of things to be downloaded using the command:\r\n",
      "#   run.py download\r\n",
      "\r\n",
      "# To add a new item to be download, add a block like this - must have 'url',\r\n",
      "# 'local_name' is optional, use to avoid name collisions\r\n",
      "\r\n",
      "#  #\r\n",
      "#  # Description of source\r\n",
      "#  #\r\n",
      "#  -\r\n",
      "#    # brief comment about file, and optionally a local_name:\r\n",
      "#    url: http://curefordisease.org/some_data.txt\r\n",
      "#    local_name: some_data_more_chars_prevent_name_collision.pdf\r\n",
      "#\r\n",
      "#  For downloading from S3 buckets, see here for information about what URL to use:\r\n",
      "#  https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingBucket.html#access-bucket-intro\r\n",
      "#  Amazon S3 virtual hosted style URLs follow the format shown below:\r\n",
      "#  https://bucket-name.s3.Region.amazonaws.com/key_name\r\n",
      "#\r\n",
      "---\r\n",
      "\r\n",
      "#\r\n",
      "# **** ROBOT ****\r\n",
      "#\r\n",
      "-\r\n",
      "  url: https://github.com/ontodev/robot/releases/download/v1.8.3/robot.jar\r\n",
      "  local_name: robot.jar\r\n",
      "-\r\n",
      "  url: https://raw.githubusercontent.com/ontodev/robot/master/bin/robot \r\n",
      "  local_name: robot\r\n",
      "\r\n",
      "# **** Ontology files ****\r\n",
      "#\r\n",
      "# ENVO\r\n",
      "#\r\n",
      "-\r\n",
      "  url: http://purl.obolibrary.org/obo/envo.json\r\n",
      "  local_name: envo.json\r\n",
      "#\r\n",
      "# CHEBI\r\n",
      "#\r\n",
      "-\r\n",
      "  url: http://purl.obolibrary.org/obo/chebi.owl.gz\r\n",
      "  local_name: chebi.owl.gz\r\n",
      "\r\n",
      "# **** Data sources ****\r\n",
      "#\r\n",
      "# Reactome \r\n",
      "# pre-transformed CHEBI to Pathway relationships\r\n",
      "#\r\n",
      "-\r\n",
      "  url: https://reactome.org/download/current/ChEBI2Reactome_PE_Pathway.txt\r\n",
      "  local_name: ChEBI2Reactome_PE_Pathway.txt\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!head download.yaml --lines=60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa48b56d",
   "metadata": {},
   "source": [
    "By default, this file instructs KGX to download 5 separate files:\n",
    "* Two files for [ROBOT](http://robot.obolibrary.org/), the Java tool used for ontology processing\n",
    "* The [ENVO ontology](https://obofoundry.org/ontology/envo.html) in JSON format\n",
    "* The [CHEBI ontology](https://obofoundry.org/ontology/chebi.html), in OWL format, and in a GZ compressed file\n",
    "* A set of mappings between CHEBI and pathways in the [Reactome knowledge base](https://reactome.org/), as a tab-delimited txt file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5400a28",
   "metadata": {},
   "source": [
    "The downloads are all stored in the `data/raw` directory.\n",
    "\n",
    "The actual download process is handled by a separate package, [kghub-downloader](https://github.com/monarch-initiative/kghub-downloader), so consult the documentation for that package to see the full extent of options you can use with `download.yaml`. It isn't limited to downloading single files from HTTP URLs: there's functionality for FTP and for retrieving the results of Elasticsearch queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6325c710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\r\n",
      "configuration:\r\n",
      "  output_directory: data/merged\r\n",
      "  checkpoint: false\r\n",
      "\r\n",
      "merged_graph:\r\n",
      "  name: project_name graph\r\n",
      "  source:\r\n",
      "    chebi:\r\n",
      "      name: \"CHEBI\"\r\n",
      "      input:\r\n",
      "        format: tsv\r\n",
      "        filename:\r\n",
      "          - data/transformed/ontologies/chebi_nodes.tsv\r\n",
      "          - data/transformed/ontologies/chebi_edges.tsv\r\n",
      "    envo:\r\n",
      "      name: \"ENVO\"\r\n",
      "      input:\r\n",
      "        format: tsv\r\n",
      "        filename:\r\n",
      "          - data/transformed/ontologies/envo_nodes.tsv\r\n",
      "          - data/transformed/ontologies/envo_edges.tsv\r\n",
      "    chebi_to_reactome:\r\n",
      "      name: \"CHEBI to Reactome Pathways\"\r\n",
      "      input:\r\n",
      "        format: tsv\r\n",
      "        filename:\r\n",
      "          - data/transformed/reactome/chebi2reactome_nodes.tsv\r\n",
      "          - data/transformed/reactome/chebi2reactome_edges.tsv\r\n",
      "  operations:\r\n",
      "    - name: kgx.graph_operations.summarize_graph.generate_graph_stats\r\n",
      "      args:\r\n",
      "        graph_name: project_name graph\r\n",
      "        filename: merged_graph_stats.yaml\r\n",
      "        node_facet_properties:\r\n",
      "          - provided_by\r\n",
      "        edge_facet_properties:\r\n",
      "          - provided_by\r\n",
      "          - source\r\n",
      "  destination:\r\n",
      "    merged-kg-tsv:\r\n",
      "      format: tsv\r\n",
      "      compression: tar.gz\r\n",
      "      filename: merged-kg\r\n",
      "    merged-kg-nt:\r\n",
      "      format: nt\r\n",
      "      compression: gz\r\n",
      "      filename: project_name.nt.gz"
     ]
    }
   ],
   "source": [
    "!head merge.yaml --lines=60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855ee551",
   "metadata": {},
   "source": [
    "As the name implies, `merge.yaml` instructs KGX to merge specific transformed data into one or more merged output graphs. The transforms place their output in `data/merged`, so that directory is specified at the top. If `checkpoint` is set to True, then each input will be converted and saved to a TSV before merging, but this isn't necessary here. We have three `source`s: the ENVO ontology, the CHEBI ontology, and the CHEBI to Reactome pathway mappings, each in nice, convenient KGX TSV format. The `operations` block defines additional processes to perform in the course of the merge. Here, a statistics file describing the merged graph's projects is generated. Finally, the `destination` block allows us to define the format(s) of the merged graph. The default file tells KGX to produce a tar.gz compressed set of KGX TSVs *and* a gz-compressed n-triple format file. \n",
    "\n",
    "Give it a try, if you haven't done so already:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d686f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Downloading files:   0%|                                  | 0/5 [00:00<?, ?it/s]\r",
      "Downloading files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 23912.79it/s]\r\n"
     ]
    }
   ],
   "source": [
    "!python run.py download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "744fd4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing data/raw/chebi.owl.gz\n",
      "[KGX][cli_utils.py][    transform_source] INFO: Processing source 'chebi.json'\n",
      "Parsing data/raw/envo.json\n",
      "[KGX][cli_utils.py][    transform_source] INFO: Processing source 'envo.json'\n",
      "Parsing data/raw/ChEBI2Reactome_PE_Pathway.txt\n",
      "Transforming using source in project_name/transform_utils/reactome/chebi2reactome.yaml\n",
      "WARNING:koza.model.config.source_config:Could not load dataset description from metadata file\n"
     ]
    }
   ],
   "source": [
    "!python run.py transform # This may take a few minutes. Take a break - you deserve it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d15574ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[KGX][cli_utils.py][               merge] INFO: Spawning process for 'chebi'\n",
      "[KGX][cli_utils.py][               merge] INFO: Spawning process for 'envo'\n",
      "[KGX][cli_utils.py][               merge] INFO: Spawning process for 'chebi_to_reactome'\n",
      "[KGX][cli_utils.py][        parse_source] INFO: Processing source 'chebi'\n",
      "[KGX][cli_utils.py][        parse_source] INFO: Processing source 'envo'\n",
      "[KGX][cli_utils.py][        parse_source] INFO: Processing source 'chebi_to_reactome'\n",
      "[KGX][graph_merge.py][       add_all_nodes] INFO: Adding 6773 nodes from envo to chebi\n",
      "[KGX][graph_merge.py][        merge_graphs] INFO: Number of nodes merged between chebi and envo: 924\n",
      "[KGX][graph_merge.py][       add_all_edges] INFO: Adding 10370 edges from <kgx.graph.nx_graph.NxGraph object at 0x7fc5616d00d0> to <kgx.graph.nx_graph.NxGraph object at 0x7fc593c84040>\n",
      "[KGX][graph_merge.py][        merge_graphs] INFO: Number of edges merged between chebi and envo: 1329\n",
      "[KGX][graph_merge.py][       add_all_nodes] INFO: Adding 15717 nodes from chebi_to_reactome to chebi\n",
      "[KGX][graph_merge.py][        merge_graphs] INFO: Number of nodes merged between chebi and chebi_to_reactome: 2324\n",
      "[KGX][graph_merge.py][       add_all_edges] INFO: Adding 92569 edges from <kgx.graph.nx_graph.NxGraph object at 0x7fc57eb005e0> to <kgx.graph.nx_graph.NxGraph object at 0x7fc593c84040>\n",
      "[KGX][graph_merge.py][        merge_graphs] INFO: Number of edges merged between chebi and chebi_to_reactome: 0\n",
      "[KGX][cli_utils.py][               merge] INFO: Merged graph has 196791 nodes and 420049 edges\n",
      "[KGX][cli_utils.py][               merge] INFO: Writing merged graph to merged-kg-tsv\n",
      "[KGX][cli_utils.py][               merge] INFO: Writing merged graph to merged-kg-nt\n"
     ]
    }
   ],
   "source": [
    "!python run.py merge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27787984",
   "metadata": {},
   "source": [
    "The merged graph will be in `data/merged`, as per the merge configuration.\n",
    "\n",
    "Let's take a quick look at `merged_graph_stats.yaml` to get an idea of what the merged graph contains. The log output of the merge will tell us how many nodes and edges the graph contains, but so will the graph stats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1666cf86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  total_edges: 420049\r\n",
      "  total_nodes: 196791\r\n"
     ]
    }
   ],
   "source": [
    "!grep total merged_graph_stats.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11298c36",
   "metadata": {},
   "source": [
    "Take a look at the list of values under `predicates` in the stats file for the list of all predicates, or look under `node_stats` to find the count of all nodes by category.\n",
    "\n",
    "The important point: **now you have a KG!** \n",
    "\n",
    "Presumably your interests extend beyond chemicals and pathways, though! In the following sections, you'll see how to customize your KG-Hub project for your own needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4ef92b",
   "metadata": {},
   "source": [
    "## Biolink basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15ff61f",
   "metadata": {},
   "source": [
    "Are you working with biological or biomedical data?\n",
    "\n",
    "No? OK, skip to the next section.\n",
    "\n",
    "Otherwise, you'll need to know at least a bit about the [Biolink Model](https://biolink.github.io/biolink-model/). Biolink defines standard types of biological data, relationships between those types, and their respective properties. All the 'things' represented as graph nodes can correspond to Biolink entities. Relationships (i.e., edges) correspond to Biolink associations. Properties of nodes or edges are termed 'slots'.\n",
    "\n",
    "Biolink *does not* fully model every type of biological phenomenon. It *does* provide enough detail to model many of the most common components of biological KGs. If you're assembling a graph of sequence variants and their disease associations, for example:\n",
    "* Sequence variant entities may be assigned the category \"[biolink:SequenceVariant](https://biolink.github.io/biolink-model/docs/SequenceVariant.html)\".\n",
    "* Variants may be associated with genotypes using the association \"[biolink:GenotypeToVariantAssociation](https://biolink.github.io/biolink-model/docs/GenotypeToVariantAssociation.html)\", where each association connects a SequenceVariant entity and an entity of the category \"[biolink:Genotype](https://biolink.github.io/biolink-model/docs/Genotype.html)\".\n",
    "* Variants may be associated with diseases using the association \"[biolink:VariantToDiseaseAssociation](https://biolink.github.io/biolink-model/docs/VariantToDiseaseAssociation.html)\", where each association connects a SequenceVariant entity and an entity of the category \"[biolink:Disease](https://biolink.github.io/biolink-model/docs/Disease.html)\".\n",
    "\n",
    "To see how this would look in practice, we'll need some data. Biolink makes some suggestions about where the source data may come from based on the identifier prefixes it knows, but these are not exhaustive. \n",
    "\n",
    "Let's begin with an association between a sequence variant and a disease, as provided by the [Orphanet rare disease knowledge base](https://www.orpha.net/). The neurodegenerative disorder [spinocerebellar ataxia type 3](https://www.orpha.net/consor/cgi-bin/OC_Exp.php?lng=EN&Expert=98757) has been found to be associated with a mutation in the *ATXN3* gene. Orphanet assigns this disease an id of ORPHA:98757, so we may use this as a unique ID for the entity. We can find a specific ID for the corresponding sequence variant by consulting the [OMIM page for this disorder](http://www.omim.org/entry/607047#0001), which tells us the [corresponding variant in the ClinVar database](https://www.ncbi.nlm.nih.gov/clinvar/variation/3551/) has an ID we can express as CLINVAR:3551. We don't really have a full genotype available for this exact variant so we'll skip modeling that association. The disease and gene nodes may be represented in KGX format as follows (note that the lines are wrapped - we have just two nodes): "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1a8155",
   "metadata": {},
   "source": [
    "```\n",
    "id      category        name    description    iri    source\n",
    "ORPHA:98757 biolink:Disease    Spinocerebellar ataxia type 3    Spinocerebellar ataxia type 3 (SCA3), also known as Machado-Joseph disease, is the most common subtype of type 1 autosomal dominant cerebellar ataxia    https://www.orpha.net/consor/cgi-bin/OC_Exp.php?Expert=98757    Orphanet\n",
    "CLINVAR:3551 biolink:SequenceVariant    NM_004993.5(ATXN3):c.892CAG[(8_36)]    \n",
    "ATXN3, (CAG)n REPEAT EXPANSION    https://www.ncbi.nlm.nih.gov/clinvar/variation/3551/    ClinVar\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8d76e3",
   "metadata": {},
   "source": [
    "Notice that we've added an [iri](https://biolink.github.io/biolink-model/docs/iri.html) and a [source](https://biolink.github.io/biolink-model/docs/source.html) to both nodes, too. We could figure out the data source from the id prefix or the IRI, but this makes it absolutely clear.\n",
    "\n",
    "The Biolink categories are hierarchical, so our entities technically belong to all the parent categories, too. All nodes can be categorized by \"biolink:NamedThing\" at least. We can add those extra categories:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f49af8",
   "metadata": {},
   "source": [
    "```\n",
    "id      category        name    description    iri    source\n",
    "ORPHA:98757 biolink:Disease|biolink:DiseaseOrPhenotypicFeature|biolink:BiologicalEntity|biolink:NamedThing    Spinocerebellar ataxia type 3    Spinocerebellar ataxia type 3 (SCA3), also known as Machado-Joseph disease, is the most common subtype of type 1 autosomal dominant cerebellar ataxia    https://www.orpha.net/consor/cgi-bin/OC_Exp.php?Expert=98757    Orphanet\n",
    "CLINVAR:3551 biolink:SequenceVariant|biolink:BiologicalEntity|biolink:NamedThing    NM_004993.5(ATXN3):c.892CAG[(8_36)]    \n",
    "ATXN3, (CAG)n REPEAT EXPANSION    https://www.ncbi.nlm.nih.gov/clinvar/variation/3551/    ClinVar\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac17e1d0",
   "metadata": {},
   "source": [
    "The next section will describe a way to add those pesky parent Biolink categories without a lot of manual labor.\n",
    "\n",
    "Let's see what the corresponding association can look like:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88797bb",
   "metadata": {},
   "source": [
    "```\n",
    "id      subject predicate       object  category        relation        source\n",
    "urn:uuid:39cf422c-9fc8-11ec-b909-0242ac120002      CLINVAR:3551      biolink:correlated_with      ORPHA:98757      biolink:Association|biolink:VariantToDiseaseAssociation      ClinVar\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2226da53",
   "metadata": {},
   "source": [
    "We've represented the relationship in a few different ways: the association in the relation column has two different categories, while there's also a *predicate* of [biolink:correlated_with](https://biolink.github.io/biolink-model/docs/correlated_with.html). [Predicate types](https://biolink.github.io/biolink-model/docs/predicates.html)\n",
    "are also standardized in Biolink but allow for more specific description than the higher-level categories.\n",
    "\n",
    "Now let's repeat the process for every Orphanet disease and every variant in ClinVar by manually identifying the associations between each and assigning them appropriate categories.\n",
    "\n",
    "Or don't do that. Your time is more valuable than that. Let's write a transform instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20c5b11",
   "metadata": {},
   "source": [
    "## How to write transforms for new sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7fd9f1",
   "metadata": {},
   "source": [
    "In a KG-Hub project, a transform is any process for turning raw data sources into KGX-format nodes and edges.\n",
    "\n",
    "This may not require much work for some data sources. For others, the data may only make sense as nodes or as edges. \n",
    "\n",
    "For all new transforms, regardless of source, you'll need to do the following:\n",
    "* Create a new directory in `project_name/transform_utils/` named for the data to be transformed. In this case, let's simulate working with a data set about [caracals](https://en.wikipedia.org/wiki/Caracal) and name the directory `caracal`. \n",
    "* In the new directory, create a Python script named something like `caracal_transform.py` as well as an `__init__.py`. The latter of these two files should contain the following:\n",
    "\n",
    "```\n",
    "from .caracal_transform import CaracalTransform\n",
    "\n",
    "__all__ = [\"CaracalTransform\"]\n",
    "    \n",
    "```\n",
    "\n",
    "* The `caracal_transform.py` will contain the actual transformation, defined as a child class of `Transform`. In this case, it will be defined as `class CaracalTransform(Transform):`.\n",
    "* In `project_name/transform.py`, add a line to import the transform and update `DATA_SOURCES`. In this case, that would mean adding\n",
    "\n",
    "```\n",
    "from project_name.transform_utils.reactome.reactome import ReactomeTransform\n",
    "```\n",
    "\n",
    "to the imports and\n",
    "\n",
    "```\n",
    "'CaracalTransform': CaracalTransform\n",
    "```\n",
    "\n",
    "to the `DATA_SOURCES`.\n",
    "\n",
    "* Add the raw inputs for the transform to `download.yaml`.\n",
    "* Add the expected outputs for the transform to `merge.yaml` so they get included in the final graph. You'll want to add something like the following under the `source:` heading:\n",
    "```\n",
    "    caracal:\n",
    "      name: \"The Illustrious Caracal Dataset\"\n",
    "      input:\n",
    "        format: tsv\n",
    "        filename:\n",
    "          - data/transformed/caracal/caracal_nodes.tsv\n",
    "          - data/transformed/caracal/caracal_edges.tsv\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afec9f9c",
   "metadata": {},
   "source": [
    "### An example transform using ontologies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba77a6a2",
   "metadata": {},
   "source": [
    "**A big note:** many pre-transformed bio-ontologies are available through the [KG-OBO](https://kg-hub.berkeleybop.io/kg-obo/) component of KG-Hub. They're already in node/edge form and therefore only require a passthrough transform. See the next section for more details.\n",
    "\n",
    "We've provided an example transform in `project_name/transform_utils/ontology/`. This code ingests two inputs: CHEBI in a gz-compressed OWL format and ENVO in OBO JSON format. Most of the activity happens in the `parse` function, where files are first decompressed if necessary, transformed into OBO JSON format if needed, then passed to KGX to be turned into node and edge files. The ROBOT utility handles JSON conversions while remaining sensitive to each ontology's structure.\n",
    "\n",
    "KGX will also happily transform files in n-triple format - just specify the input format as 'nt'.\n",
    "\n",
    "The rest is up to you - different data files will require different solutions, especially if they are in proprietary formats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62fb7d2",
   "metadata": {},
   "source": [
    "### Passthrough transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a21a7a",
   "metadata": {},
   "source": [
    "Why write an entirely new transform when the nodes and edges you need are ready to use? All you need is a way to move the right files to the right directory. A passthrough transform will do that.\n",
    "\n",
    "As mentioned above, [KG-OBO](https://kg-hub.berkeleybop.io/kg-obo/) stores bio-ontology graphs. If you really wanted to incorporate the Ontology of Biological and Clinical Statistics (OBCS) into your nascent KG, you can easily retrieve OBCS from https://kg-hub.berkeleybop.io/kg-obo/obcs/2018-02-22/obcs_kgx_tsv.tar.gz. You may set up a transform like any other, complete with including it in `download.yaml`. In the parse function, however, simply pass the data file to KGX as:\n",
    "```\n",
    "        transform(inputs=[data_file],\n",
    "                  input_compression='tar.gz',\n",
    "                  input_format='tsv',\n",
    "                  output=os.path.join(self.output_dir, name),\n",
    "                  output_format='tsv')\n",
    "```\n",
    "\n",
    "KGX will still run some basic sanity-checking validation on the input. The nodes and edges will emerge, ready to be included in a merge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16634958",
   "metadata": {},
   "source": [
    "### Writing transforms with Koza"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb2ca6e",
   "metadata": {},
   "source": [
    "The [Koza](https://github.com/monarch-initiative/koza) package is a great way to write transforms without getting stuck in a loop of character-level file parsing anxiety. Koza allows definition of specific data elements in transform-specific configuration files. Entities and relationships may be ingested according to their Biolink classes, which has the added benefit of validating their properties (e.g., it will tell you when identifier formats look incorrect).\n",
    "\n",
    "The example Reactome transform provided in the template (see `project_name/transform_utils/reactome/`) incorporates Koza. The [full Koza documentation](https://koza.monarchinitiative.org/) will also be helpful as the project is constantly under improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ff5f8b",
   "metadata": {},
   "source": [
    "## How and where to store results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561f21b0",
   "metadata": {},
   "source": [
    "Storage is available on [KG-Hub](https://kg-hub.berkeleybop.io/). Contact [Harry Caufield](mailto:jhc@lbl.gov) or [Justin Reese](mailto:justaddcoffee@gmail.com) for more details and to secure space for your project. You will need to have your project code in a publicly accessible repository as described above in **KG-Hub basics**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7f12c0",
   "metadata": {},
   "source": [
    "### Jenkins builds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a6b167",
   "metadata": {},
   "source": [
    "The whole process of downloading, transforming, and merging may be automated with your choice of platform. We prefer [Jenkins](https://www.jenkins.io/) and have provided an example Jenkinsfile (the file describing the entire automation process from start to finish, as well as the indicator to Jenkins that a build process should be performed) in the template repository. This example Jenkinsfile *will not work* without some specific edits: \n",
    "* Its title will need to be changed to 'Jenkinsfile' from 'JenkinsfileEXAMPLE'.\n",
    "* Jenkins works well with Docker images, so you may have control over the software environment. An image name may be provided in line 5.\n",
    "* This configuration assumes the final graph products will be uploaded to an AWS S3 bucket. The bucket name should be specified for `S3BUCKETNAME` in line 13. The directory path on the bucket, not including the bucket name, should be specified for `S3PROJECTDIR` in line 14.\n",
    "* Change the value of `MERGEDKGNAME_BASE` to your project name in line 15.\n",
    "* Change the repository URL to that of your project in line 62.\n",
    "\n",
    "You will also need to have access to a Jenkins platform on its own server and will need to provide Jenkins with one (or more) credential files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c02fa2a",
   "metadata": {},
   "source": [
    "## Next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f893b537",
   "metadata": {},
   "source": [
    "See the other notebook in this repository, 'Machine Learning on Knowledge Graphs'.\n",
    "\n",
    "Happy graph-making!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
